\section*{Supplemental materials}

% \section{Appendices: additional results}

% reset figure/table counters so they start at “1” again
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{algorithm}{0}
\setcounter{section}{0}

% make sections A, B, C, … instead of 1, 2, 3, …
\renewcommand{\thesection}{\Alph{section}}

% have figures and tables numbered A1, A2, … under Section A;
% B1, B2, … under Section B; etc.
\counterwithin{figure}{section}
\counterwithin{table}{section}
\counterwithin{algorithm}{section}
\renewcommand{\thealgorithm}{\thesection\arabic{algorithm}}
\renewcommand{\thefigure}{\thesection\arabic{figure}}
\renewcommand{\thetable}{\thesection\arabic{table}}
\section{Traditional Bootstrap Example}
\label{sec:boot-fail}

Figure~\ref{Fig:boot-fail} shows the coverage as a function of the value of $\beta$ (solid line) and the average coverage (dashed line) using a traditional pairs bootstrapping approach on the simulation setup described in Section~\ref{Sec:coverage}. Compare to the right side of Figure~\ref{Fig:laplace}.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.7\linewidth]{figureA1}
  \caption{\label{Fig:boot-fail} Corresponds to the setup used in the right side of Figure~\ref{Fig:laplace}, but using a traditional bootstrapping approach.}
  \end{center}
\end{figure}

\newpage

\section{Sampling from the Full Conditional Posterior}\label{Sup:A}

Here we assume $\X$ has been standardized s.t. $\x_j^T\x_j = n$. Define $\Q_{\hat{S}_j}$ as $\I - \X_{\hat{S}_j}(\X_{\hat{S}_j}^T \X_{\hat{S}_j})^{-1} \X_{\hat{S}_j}^T$, the projection matrix onto the features selected by the lasso. For $\beta_j$ conditional on the selected features

\as{
  \begin{aligned}
  L(\beta_j|\hat{S}_j) &\propto \exp(-\frac{\tilde{n}}{2\sigma^2}(\beta_{j}^2 - 2\tilde{\beta}_{j}\beta_{j})), \\
  \end{aligned}
}

where $\tilde{\beta}_j = (\x_j^T \Q_{\hat{S}_j} \x_j)^{-1} \x_j^T \Q_{\hat{S}_j} \y$ and $\tilde{n} = \x_j^T \Q_{\hat{S}_j} \x_j$.

The lasso can be formulated as a Bayesian regression model with a laplace (double exponential) prior. In this case, the prior for $\beta_j$ is proportional to $\exp(-\frac{\tilde{n} \lambda} {\sigma^2} \abs{\beta_j})$. This prior ensures that the meaning of $\lam$ is maintained.

With this the form of the full conditional posterior can be worked out as follows:
\as{
p(\beta_j | \hat{S}_j) &\propto \exp(-\frac{\tilde{n}}{2\sigma^2} (\beta_j^2 - 2\tilde{\beta}_j\beta_j)) \exp(-\frac{\tilde{n} \lambda} {\sigma^2} \abs{\beta_j}) \\
&= \exp(-\frac{\tilde{n}}{2\sigma^2} (\beta_j^2 - 2 \tilde{\beta}_{j}\beta_j +  2 \lambda \abs{\beta_j})) \\
&= \exp(-\frac{\tilde{n}}{2\sigma^2} (\beta_j^2 - 2(\tilde{\beta}_{j}\beta_j - \lambda \abs{\beta_j}))) \\
&=
\begin{cases}
\exp(-\frac{\tilde{n}}{2\sigma^2} (\beta_j^2 - 2(\tilde{\beta}_{j} + \lambda)\beta_j)), \text{ if } \beta_j < 0, \\
\exp(-\frac{\tilde{n}}{2\sigma^2} (\beta_j^2 - 2(\tilde{\beta}_{j} - \lambda)\beta_j )), \text{ if } \beta_j \geq 0 \\
\end{cases} \\
&\propto
\begin{cases}
C_{-} \exp\{-\frac{\tilde{n}}{2\sigma^2} (\beta_j - (\tilde{\beta}_j + \lambda))^2\}, \text{ if } \beta_j < 0, \\
C_{+} \exp\{-\frac{\tilde{n}}{2\sigma^2} (\beta_j - (\tilde{\beta}_j - \lambda))^2\}, \text{ if } \beta_j \geq 0 \\
\end{cases}
}
where $C_{-} = \exp(\tilde{\beta}_j \lambda \tilde{n}/\sigma^2)$ and $C_{+} = \exp(-\tilde{\beta}_j \lambda \tilde{n}/\sigma^2)$.

Note the piecewise defined posterior is made up of a kernel of two normal distributions. This can be leveraged and draws can be efficiently obtained through a mapping onto the respective normal distributions. To define this mapping, it helps to introduce a concept and some notation. First, the use of ``tails'' here refers to the entirety of a distribution between 0 and $\pm \infty$. That is, the lower tail is any part of the distribution below 0 and the upper tail is any part greater than 0, therefore $P(X \in lower \cup X \in upper) = 1$. Accordingly, we will let the tail probabilities in each of the two normals to transformed on to be denoted $Pr_{-}$ and $Pr_{+}$ respectively and the probability in each of the tails of the posterior, denoted $Post_{-}$ and $Post_{+}$ respectively. $Pr_{\pm}$ is trivial to compute with any statistical software. $Post_{\pm}$ is conceptually simple, although care must be taken to avoid numerical instability. With this notation in place, note that,
\as{
p(\beta_j | \hat{S}_j)  & \propto
\begin{cases}
C_{-} Pr_{-}, \text{ if } \beta_j < 0, \\
C_{+} Pr_{+}, \text{ if } \beta_j \geq 0\\
\end{cases}
} which implies that $Post_- = \frac{C_{-} Pr_{-}}{C_{-} Pr_{-} + C_{+} Pr_{+}}$ and similarly for $Post_+$. To avoid numerical instability, or rather to handle it properly when it is unavoidable, we will work on the $\log$ scale. This works well for most of the problem, but computation of $Post_-$ and $Post_+$ need something a bit more since, for example, $\log(Post_-) = \log(C_{-}Pr_{-}) - \log(C_{-} Pr_{-} + C_{+} Pr_{+})$. That is, the denominator still must be computed then the $\log$ taken which does not allow operating on the $\log$ scale to fully address potential numerical instability. Instead, let $\ell_{-} := \log C_{-} + \log Pr_{-}, \ell_{+} := \log C_{+} + \log Pr_{+}$, and $\Delta := \ell_{-} - \ell_{+}$, then $\log(Post_-)$ can be computed with $\Delta - \log\bigl(1 + \exp(\Delta)\bigr)$. This still doesn't completely address the issue, however, if $\exp(\Delta)$ is infinite then $C_-Pr_- >> C_+Pr_+$ and $\log(Post_-) \approx 0$ which means $Post_- \approx 1$ (equivalently $Post_+ \approx 0$).

With these values, we can compute the quantile by mapping the corresponding probability $p$ for the posterior onto the probability $p^*$ for the corresponding normals. Which normal the quantile of interest ultimately comes from is determined based on $Post_{\pm}$. If $p \leq Post_{-}$, then $p$ would be mapped onto the negative normal. If $p > Post_{-}$, then $p$ would be mapped onto the positive normal.  For example, if $Post_{+} = 0.98$ and $p = 0.1$ then $p$ would be mapped onto the positive normal. The transformation to map a given probability from the posterior depends on which tail the quantile resides in on the posterior (equivalently which normal it is being mapped to, the positive or negative). This map is simply:

\as{
p^* &= p \times (Pr_{\pm} / Post_{\pm}) \\
}


Once the respective probability is mapped, one can simply use the inverses of the normal CDF that the probability was mapped to. That being said, there is a nuance worth pointing out. When transforming the probabilities, the step to determine which tail the respective quantile comes from occurs first. With this, the probability should be adjusted so that it refers to the probability between the quantile of interest and the respective tail. After this, then the transformation can be applied. These steps are summarized in Algorithm~\ref{alg:quantile}.

\begin{algorithm}
\caption{Compute Quantile for RL-P Laplace-Normal Distribution}
\label{alg:quantile}
\begin{algorithmic}[1]
  \Require
    $\tilde{\beta}_j,\;\sigma^2,\;\tilde{n};\lambda,\;p$ \Comment{mean, variance, sample size, penalty, target significance level}
  \Statex
  \State // 1. Compute prior mass for negative and positive regions (on log‐scale)
  \State $Pr_{-} \gets \Phi\bigl(0;\,\tilde{\beta}_j + \lambda,\;\tfrac{\sigma^2}{\tilde{n}}\bigr)$
  \State $Pr_{+} \gets 1 - \Phi\bigl(0;\,\tilde{\beta}_j - \lambda,\;\tfrac{\sigma^2}{\tilde{n}}\bigr)$
  \Statex
  \State // 2. Compute posterior weights $Post_{-},\,Post_{+}$ with log‐scale stabilization
  \State $\ell_{-} \gets \log C_{-} + \log Pr_{-}$
  \State $\ell_{+} \gets \log C_{+} + \log Pr_{+}$
  \State $\Delta \gets \ell_{-} - \ell_{+}$
  \If{$\exp(\Delta) = \infty$}
    \State $\log Post_{-} \gets 0$  \Comment{since $C_-Pr_- \gg C_+Pr_+$}
  \Else
    \State $\log Post_{-} \gets \Delta - \log\bigl(1 + \exp(\Delta)\bigr)$
  \EndIf
  \State $Post_{-} \gets \exp(\log Post_{-})$
  \State $Post_{+} \gets 1 - Post_{-}$
  \Statex
  \State // 3. Invert CDF on the appropriate component
  \If{$p \le Post_{-}$}
    \State $w \gets \dfrac{Pr_{-}}{Post_{-}}$
    \State $q \gets \Phi^{-1}\bigl(p \,w;\;\tilde{\beta}_j + \lambda,\;\tfrac{\sigma^2}{n}\bigr)$
  \Else
    \State $w \gets \dfrac{Pr_{+}}{Post_{+}}$
    \State $q \gets \Phi^{-1}\!\Bigl(1 - (1-p)\,w;\;\tilde{\beta}_j - \lambda,\;\tfrac{\sigma^2}{n}\Bigr)$
  \EndIf
  \State \Return $q$
\end{algorithmic}
\end{algorithm}

\newpage

\section{Coverage Behavior Under Alternative Sample Sizes}\label{Sup:alt_ns}

Figure~\ref{Fig:coverage_by_n} displays coverage estimates as a smooth function of $\beta$ for three values of n: 50, 100, and 400 but otherwise uses the same setup as the simulation described in Section~\ref{Sec:coverage}.

\begin{figure}[hbtp]
  \begin{center}
  \includegraphics[width=0.35\linewidth]{figureC1}
  \caption{\label{Fig:coverage_by_n} The results displayed are from a simulation with the same set up as in Section~\ref{Sec:coverage} but with n set to three different values: 50, 100, 400. The fitted curves are from Binomial GAMs fit with coverage being modeled as a smooth function of $\beta$. The dashed lines represent the average coverages across all 1000 independently generated datasets and the solid black line indicates the 80\% nominal coverage rate.}
  \end{center}
\end{figure}

For $n = 50$, coverage is overconservative with the characteristic high coverage for values near zero and low coverage for larger values of $\beta$. However, As $n$ is increased to 100 then 400 the characteristic over coverage for small $\beta$ values and under coverage for large $\beta$ values lessens, largely attributable to $\lam_{\CV}$ being smaller.

\newpage

\section{Selective Inference Intervals}\label{Sup:si_int_info}


\begin{table}[hb]
  \singlespace
  \centering
  \input{code/out/tableD1}
  \caption{Additional information on the results for Selective Inference in the simulation described in Section~\ref{Sec:Comparison}.}
  \label{Tab:selective_inference}
\end{table}


Because Selective Inference only provides intervals for the subset of parameters with nonzero coefficients, the results in Figures~\ref{Fig:laplace_comparison}~and~\ref{Fig:laplace_other} are just for this subset. This is in contrast to the results for de-sparsified lasso and RL-P which are over all parameters. The first two columns of Table~\ref{Tab:selective_inference} provide more details on the size of the lasso models selected by cross validation, and hence, on the number of intervals constructed. The first column indicates how many simulations (out of 1000) the intercept-only model was selected, in which case no intervals are produced. The second column gives the average number of selected parameters (inclusive of when none were selected). It is the subset of parameters represented by column 2 that are the results in Figures~\ref{Fig:laplace_comparison}~and~\ref{Fig:laplace_other} are from.

For the intervals that were constructed, the third and fourth columns provide the number of simulations that had a infinite median width or any interval with an infinite width, respectively, features that were not evident in Figure~\ref{Fig:laplace_other}. Note that simulations where the null model was chosen by definition can not have intervals of infinite width.

\newpage


\section{MCP Example}\label{Sup:MCP}

\begin{table}[hbtp]
  \centering
  \input{code/out/tableE1}
  \caption{\label{Tab:epsilon_conundrum_coverage} Coverage rates by magnitude of $\beta$ for RL-P using both the lasso and the MCP penalty approximation applied to the Sparse 1 scenario described in Section~\ref{Sec:distribution}. The nominal coverage rate is 80\%. }
\end{table}

The results here provide an example of a version of the RL-P method but with an approximation to the MCP penalty to obtain intervals. The MCP penalty closely resembles that of the lasso near zero but eventually levels out to a constant penalty for larger values of $\beta$ unlike the lasso which applies a penalty proportional to the magnitude of $\beta$. Note that although $\beta$s with intermediate magnitudes are still under covered (albeit with coverage notably higher than RL-P lasso), that the largest $\beta$s have coverage at nearly 70\%, over doubling the coverage of the RL-P lasso.

\newpage

\section{Additional Details on Bootstrap Bias}\label{Sup:proof}

We start with a simple proof of the additional bias introduced by bootstrapping in one and two dimensional settings starting first with Ridge regression and then the lasso. After these simple proofs, we provide a simulation that helps generalize this issue to high dimensional settings. Note that here the goal is to provide an intuitive understanding of the issue. Others have already proved this issue more generally \citep{karoui2016, clarté2024}, however, we feel that the lack of straight forward examples may be in part why the bootstrap is still a tool used for penalized regression in general.

Let $\frac{1}{n}x_1^Tx_1 = s_{11}$, and $s_{11}^*$ be the bootstrapped version of $s_{11}$. Furthermore more that $E_{boot}[s_{11}^*] = s_{11}$.

For Ridge, it can be shown that $E(\bh_1) = \frac{s_{11}}{s_{11} + \lam}\beta_1$, letting $g(s_{11}) = \frac{s_{11}}{s_{11} + \lam}$ note that:

$$
\begin{aligned}
g'(s_{11}) &= \lambda (s_{11} + \lam)^{-2} \\
g''(s_{11}) &= -2\lambda (s_{11} + \lam)^{-3} < 0
\end{aligned}
$$

Thus by Jensen's inequality we have,

$$
E_{boot}[g(s_{11}^*)] \leq g(E_{booot}[s_{11}^*]) = g(s_11).
$$

Multiplying by $\beta_1$ then gives,

$$
E_{boot}[\bh_1^*] = E_{boot}[g(s_{11}^*)]\beta_1 \leq g(s_{11})\beta_1 = E[\bh_1].
$$

That is, even in a single parameter setting, just due to the bootstrap variability of $s_{11}$ alone, we would expect bias in the bootstrapped estimate of $\beta_1$.

Now consider a two parameter setting where we assume $\beta_2 = 0$. Let $V = \frac{1}{n}\boldsymbol{X}^T\boldsymbol{X} = \begin{pmatrix} s_{11} & s \\ s & s_{22} \end{pmatrix}$.

Similarly here, one can find that

$$
\begin{aligned}
E[\hat{\beta}_1 | V, \lambda] &= \frac{s_{11}(s_{22} + \lambda) - s^2}{(s_{11} + \lambda)(s_{22} + \lambda) - s^2} \beta_1 \\
&= g(s_{11}, s_{22}, s)\beta_1
\end{aligned}
$$


Furthermore, with some patience, it is possible to work out the Hessian of $g$:

$$
H = \nabla^{2}g
= \frac{\lambda}{D^{3}}
  \begin{pmatrix}
    -2(s_{22}+\lambda)^{3} & -2(s_{22}+\lambda)s^{2} & 4(s_{22}+\lambda)^{2}s\\[6pt]
    -2(s_{22}+\lambda)s^{2} & -2s^{2}(s_{11}+\lambda) & 2(s_{11}+\lambda)(D+2s^{2})s\\[6pt]
     4(s_{22}+\lambda)^{2}s &  2(s_{11}+\lambda)(D+2s^{2})s & -2(s_{22}+\lambda)(A+3s^{2})
  \end{pmatrix}.
$$

Because $\lambda/D^{3}>0$, negative semidefiniteness of $H$ is equivalent to that of the scaled matrix $\tilde H = D^{3}H/\lambda$.

$$
\tilde H_{11} = -2(s_{22}+\lambda)^{3} < 0
$$
$\det\bigl(\tilde H_{[1:2,1:2]}\bigr)$:
$$
\begin{aligned}
\det\bigl(\tilde H_{[1:2,1:2]}\bigr) &= \det \begin{pmatrix}
  -2(s_{22}+\lambda)^{3} & -2(s_{22}+\lambda)s^{2}\\[4pt]
  -2(s_{22}+\lambda)s^{2} & -2s^{2}(s_{11}+\lambda)
\end{pmatrix}\\[8pt]
&= 4(s_{22}+\lambda)^{2}s^{2}D \geq 0 \\
\end{aligned}
$$

and

$$
\det(\tilde H) = -\,8\,\lambda\,(s_{22}+\lambda)^{3}s^{2}\,(s_{11}+\lambda)\,D \leq 0.
$$

As such, $H$ is negative‐semidefinite and consequently $g(s_{11},s_{22},s)$ is globally concave in all three arguments. Now, let $V^*$ be the bootstrapped version of V and note that $E_{boot}[(s_{11}^*, s_{22}^*, s^*)] = (s_{11}, s_{22}, s)$. By Jensen's inequality:

$$
E_{boot}[g(S^*)] \leq g(E_{boot}[S^*]) = g(S).
$$

Multiplying by $\beta_1$ then gives:

$$
E_{boot}[\hat{\beta}_1^*] = E_{boot}[g(S^*)]\beta_1 < g(S)\beta_1 = E[\hat{\beta}_1].
$$

A similar argument can be made with the lasso, however, the mathematical details become more involved. Whereas with ridge we started off assuming the true values, with lasso it is easier to work with assumed conditions on the lasso estimates themselves as they directly affect the KKT conditions. Starting with a single parameter set up, assume that $\bh_1 > 0$. Then,

$$
\begin{aligned}
\frac{1}{n}\x_1^T(\y - \x_1 \bh_1) &= \lambda \\
\bh_1 &= \frac{1}{ns_{11}}\x_1^T\y - \frac{\lambda}{s_{11}} \\
E[\bh_1] &= \beta_1 - \frac{\lambda}{s_{11}} \\
\end{aligned}
$$

Let $h(s_{11}) = - \frac{\lambda}{s_{11}}$, then

$$
\begin{aligned}
h'(s_{11}) &= \frac{\lambda}{s_{11}^2}, \\
h''(s_{11}) &= \frac{-2\lambda}{s_{11}^3}  < 0\\
\end{aligned}
$$

and $h(s_{11})$ is therefore concave. So by Jensen's inequality

$$
\begin{aligned}
E_{boot}[h(s_{11}^*)] \leq h(E_{booot}[s_{11}^*]) = h(s_11).
\end{aligned}
$$

Adding $\beta_1$ then gives,

$$
\begin{aligned}
E_{boot}[\bh_1^*] = \beta_1 + E_{boot}[h(s_{11}^*)] \leq \beta_1 + g(s_{11}) = E[\bh_1].
\end{aligned}
$$

Now consider a two parameter setting. If we assume $\hat{\beta}_1 > 0$ and $\hat{\beta}_2 = 0$, bias can only be guaranteed when $\beta_2 = 0$. This boils down to the same details as the single parameter case we just considered. If both $\hat{\beta}_1$ and $\hat{\beta}_2$ are assumed positive, we arrive at the following KKT conditions:

$$
\begin{aligned}
\frac{1}{n}\boldsymbol{X}^T\boldsymbol{X} \hat{\boldsymbol{\beta}} &= \frac{1}{n}\boldsymbol{X}^Ty - \lambda 1_2 \\
\end{aligned}
$$

Again, let $\boldsymbol{V} = \frac{1}{n}\boldsymbol{X}^T\boldsymbol{X} = \begin{pmatrix} s_{11} & s \\ s & s_{22} \end{pmatrix}$, then

$$
\begin{aligned}
\hat{\beta_1} = \frac{s_{22}(x_1^Ty- n\lambda) - s(x_2^Ty- n\lambda)}{n(s_{11}s_{22} - s^2)}
\end{aligned}
$$

and

$$
\begin{aligned}
E(\hat{\beta}|X) &= \beta_1 - \lambda\frac{s_{22} - s}{s_{11}s_{22} - s^2} \\
&= \beta_1 + h(s_{11}, s_{22}, s)
\end{aligned}
$$

Differentiation gives
$$
\nabla^{2}h \;=\; \frac{\lambda}{D^{3}}
\begin{pmatrix}
-2s_{22}^{2}(s_{22}-s) & -2s_{22}s^{2} & 4s_{22}^{2}s\\[6pt]
-2s_{22}s^{2} & -2s_{11}s^{2} & 2\,s_{11}s\bigl(D+2s^{2}\bigr)\\[6pt]
4\,s_{22}^{2}s & 2\,s_{11}s\bigl(D+2s^{2}\bigr) & -2s_{22}\bigl(s_{11}s_{22}-s^{2}+3s^{2}\bigr)
\end{pmatrix}.
$$


Leading (1×1) minor:
  $$
    H_{11} = -\frac{2\lambda s_{22}^{2}(s_{22}-s)}{D^{3}} < 0
  $$

Leading (2×2) minor:
  $$
    \det\bigl(H_{[1:2,1:2]}\bigr)
      = \frac{4\lambda^{2}s_{22}^{2}s^{2}D}{D^{6}} \geq 0.
  $$

Full determinant:
  $$
    \det(H)
      = -\frac{8\lambda^{3}\,s_{22}^{3}s^{2}s_{11}\,s_{11}D}{D^{9}} \leq 0.
  $$

The Hessian is negative‐semidefinite.  Therefore the map $g(s_{11},s_{22},s)$ is jointly concave, and multivariate Jensen's inequality implies
$$
E_{\text{boot}}\bigl[\hat\beta_{1}^{*}\bigr]
  \;\le\; E\bigl[\hat\beta_{1}\mid X,\,z_{1}=z_{2}=+1\bigr],
$$
so the bootstrap mean of $\hat\beta_{1}$ sits strictly closer to zero than the original estimate whenever both fitted coefficients are positive. Note that in each of these settings a common thread is that the curvature, which affects the amount of bias introduced by the bootstrap is heavily dependent on $\lambda$. Thus, the larger $\lambda$ is the more the bootstrap will be biased. We also see in the two parameter setting that the correlation between features has a considerable role in the curvature. How much each of these contribute to the bootstrap bias is problem dependent, however, our exploration, such as the example we provide next, suggests that $\lambda$ is likely the larger influencer in general. That said, full exploration is outside the scope of this work, but could be explored by considering the eigenvalues for the hessians above under various scenarios.

How does this extend to high dimensions? To answer this question, we consider the following simulation study.

In order to increase interpretability, we consider a simplified scenario here. Consider a set up with $n = p = 100$ where there is 1 true non-null variable, $A$ s.t. $\beta_A = 2$ that is correlated with a null variable $B$ with $\rho = 0.5$. All other variables are generated independent of $A$ and each other. For this set up, $A$ is always selected to be in the model, both with the original data and for all bootstrap replications at $\lam_{CV}$. This is important as decomposing the bias is more complicated for variables that are not selected to be in the model.

Note, the bias for $\bh_j \neq 0$ is $\frac{1}{n}\x_j^T \epsilon + \frac{1}{n}\x_j^T \X_{-j}(\bb^*_{-j} - \bbh_{-j}) + \lam$. We can break this down further to apply to the scenario outlined above. $Bias_A = \frac{1}{n}\x_A^T \epsilon + \frac{1}{n}\x_A^T \x_{B}(\beta^*_{B} - \bh_{B}) + \frac{1}{n}\x_A^T \X_{N}(\bb^*_{N} - \bbh_{N}) + \lam$. That is, we can decompose the bias into four parts. The first is the irreducible bias which comes from the chance correlation between $\x_A$ and the errors. The last is the bias directly introduced by the lasso penalty. The other two components attribute bias from the single $B$ variable and all 98 $N$ variables respectively. By taking the simulation set up in the previous paragraph and repeating it 1000 times and each time saving the bias attributable to each of the components, we can get an idea of the distribution of the bias components. More specifically, for each generated dataset, we can decompose the bias for the estimates from the original data as well as from the 1000 bootstrap replications. For the bootstrap replications, we can then save the mean bias. Figure~\ref{Fig:bias_decomp_single_B} shows a summary from doing just that. The top panel gives the densities of the mean bootstrap biases across the 1000 repetitions, the middle gives the densities for the biases on the original dataset across the 1000 repetitions, and the bottom give the densities of their paired differences. In this depiction, the contribution of $\lam$ is excluded. Additionally note here that a positive bias is used to indicated bias \emph{towards} zero.

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{figureF1}
    \caption{\label{Fig:bias_decomp_single_B} n = p = 100, $\beta_A = 2$, $\beta_B = 0$, $\rho_{A,B} = 0.5$.  All other $\beta$s = 0 and  generated under independence}
    \end{center}
\end{figure}

While the derivation above suggests that increasing correlation is the main contributor to increased bootstrap bias, we see that it is actually the cumulative effect of the 98 N variables that drives the bias rather than the correlation with variable $B$. This has important implications in that even when overall correlations are low, that the sparsity in high dimensional settings is going to lead to a significant bootstrap bias. It is important to differentiate this bias and the bias inherent in the motivation for Section~\ref{Sec:IAC}. The framework put forth in Section~\ref{Sec:IAC} allows for the bias introduced by penalization which we are arguing is permissible in this newly suggested coverage framework, however, the additional bootstrap bias here is what inherently leads to the breakdown of the bootstrap even for average coverage.

\newpage
\section{A note about stability selection}
\label{sec:stability}

It is no secret that bootstrapping lasso has fundamental issues. The related work in this manuscript is simply meant to show easy to comprehend details to this end. With these issues in mind, it is not common to see a traditional bootstrapping approach applied to the lasso for the purposes of interval construction. Rather, the popular use of resampling techniques for the lasso is in stability selection introduced by  \cite{Meinshausen2010}. However, here, we show that stability selection is still affected by bootstrap bias.

Consider the following set up. $n = 50$, $p = 500$, 4 $\beta$s contain signal: $\beta_{1-4} = (0.25, 0.5, 1, 2)$ and the rest are zero. $\X$ were generated independently from a $\Norm(0, 1)$. Finally, $\y$ was generated as $\y = \X\bb + \bvep$, where $\veps_i \iid N(0, 1)$. Using this data setup, stability selection was performed as outlined in Algorithm~\ref{alg:singlelambda}.

\begin{algorithm}[!ht]
\caption{Bootstrap Stability Selection at a \emph{single} CV‐chosen $\lambda$}
\label{alg:singlelambda}
\begin{algorithmic}[1]
\Require\ $R$ replicated data sets, $B$ bootstraps per data set,
          $p$ predictors
\Statex \textbf{Let:}
  $\mathbf A \in\{0,1\}^{R\times p}$, $\mathbf A^{\!*}\in[0,1]^{R\times p}$
%
\For{$i=1,\dots,R$}
  \State Generate $(\mathbf X,\mathbf y)$ from the data-generating process
  \State $\lambda_{\text{CV}}\leftarrow
         \operatorname*{arg\,min}_{\lambda}\text{CV‐Error}(\lambda)$
         \Comment{10-fold CV on $(\mathbf X,\mathbf y)$}
  \State Obtain lasso estimates at $\lambda_{\text{CV}}$
        and save
        $\mathbf A_{i,\cdot}\gets\bigl[\hat\beta(\lambda_{\text{CV}})\neq 0\bigr]$

  \State Initialise $\mathbf B^{\!*}\gets\mathbf 0_{B\times p}$
  \For{$b=1,\dots,B$}                        \Comment{pairs bootstraps}
     \State Draw indices $\mathcal I_b$ with replacement from $\{1,\dots,n\}$
     \State $(\mathbf X^{b},\mathbf y^{b})\gets
            (\mathbf X_{\mathcal I_b,\cdot},\mathbf y_{\mathcal I_b})$
     \State Fit lasso on $(\mathbf X^{b},\mathbf y^{b})$ at $\lambda_{\text{CV}}$
     \State $\mathbf B^{\!*}_{b,\cdot}\gets
            \bigl[\hat\beta^{\!*}(\lambda_{\text{CV}})\neq 0\bigr]$
  \EndFor
  \State $\mathbf A^{\!*}_{i,\cdot}\gets
         \dfrac1B\sum_{b=1}^{B}\mathbf B^{\!*}_{b,\cdot}$
\EndFor
\end{algorithmic}
\end{algorithm}

After we computed $\bar{\mathbf A}\gets\dfrac1R\sum_{i=1}^{R}\mathbf A_{i,\cdot}$,  \; $\bar{\mathbf A}^{\!*}\gets\dfrac1R\sum_{i=1}^{R}\mathbf A^{\!*}_{i,\cdot}$ and then compare $\bar{\mathbf A}$ (prob.\ of being selected in the original fit) with $\bar{\mathbf A}^{\!*}$ (expected bootstrap stability) to evaluate how well the inclusion probabilities mirror single-$\lambda$ selection behavior. The results are presented in Table~\ref{Tab:stability_selection}.

\begin{table}[hbtp]
  \centering
  \input{code/out/tableG1}
  \caption{\label{Tab:stability_selection} Results for simulation described in Section~\ref{sec:stability} showing that stability selection also suffers from bootstrap bias. Original selection(\%) provides the empirical selection probabilities for the 4 parameters while Bootstrap stability gives the bootstrap estimate for selection.}
\end{table}

Clearly, there is a fundamental bias for stability selection. That said, further exploration is necessary to understand if this contradicts the claims of \cite{Meinshausen2010} as they primarily focus on FDR control for finite sample problems like the one considered here.

Note, this implementation deviates from the proposed stability selection algorithm originally introduced by \cite{Meinshausen2010} in order to retain a connection to the bootstrap implementation in this manuscript, however, even when considering the entire lasso path fit on sub-bagged samples the bias remained largely the same.
